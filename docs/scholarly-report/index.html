<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport"
      content="width=device-width, initial-scale=1.0, user-scalable=yes">
    <title>Scholarly HTML</title>
    <link rel="stylesheet" href="css/scholarly.min.css">
    <script src="js/scholarly.min.js"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
          const biblioSection = document.querySelector('section#biblio-references');
          if (biblioSection) {
              biblioSection.remove();
          }
      });
    </script>
    <style>
      pre code {
          width: 900px;
          background-color: #eee;
          border: 1px solid #999;
          display: block;
          padding: 20px;
      }
  </style>
  </head>
  <body prefix="schema: http://schema.org">
    <header>
      <div class="banner">
        <img src="scholarly-html.svg" width="227" height="50"
          alt="Scholarly HTML logo">
        <div class="status">Community Draft</div>
      </div>
      <h1>Web News Provenance</h1>
    </header>
    <div role="contentinfo">
      <dl>
        <dt>Authors</dt>
        <dd>
          Opariuc Rares-Ioan :
          <a href="https://github.com/OpariucRares/">Github</a>
          &amp;
          Tablan Andrei Razvan :
          <a href="https://github.com/andreitablan/">Github</a>
        </dd>
      </div>
      <section typeof="sa:Abstract" id="abstract" role="doc-abstract">
        <h2>Abstract</h2>
        <p>
          This project develops a web application to model and manage the
          provenance
          of online newspaper articles, including textual and multimedia
          content.
          It utilizes metadata standards like DCMI, IPTC, and the Social
          Semantic Web Thesaurus,
          with additional data from DBpedia and Wikidata. A SPARQL endpoint
          exposes RDFa and
          JSON-LD data, enhancing access to knowledge such as fresh editorials
          and categorization.
        </p>
      </section>
      <section id="introduction" role="doc-introduction">
        <h2>Introduction</h2>
        <p>
          This report provides an in-depth technical overview of Web News
          Provenance (NePr), an advanced web platform
          designed to track, model, and manage the provenance of online
          newspaper articles, including multimedia content
          and language variants. The platform is built on a service-oriented
          architecture, delivering a suite of functionalities
          aimed at improving content accessibility and integrity through
          comprehensive metadata standards.
        </p>
        <p>
          Leveraging widely-recognized classification models like
          <a href="https://www.dublincore.org/specifications/dublin-core/"
            target="_blank">DCMI</a>,
          <a href="https://iptc.org/standards/" target="_blank">IPTC</a>, and
          the
          <a href="http://vocabulary.semantic-web.at/semweb.html"
            target="_blank">Social Semantic Web Thesaurus</a>,
          the platform integrates additional metadata from
          <a href="https://www.wikidata.org/wiki/Wikidata:Main_Page"
            target="_blank">Wikidata</a> and
          <a href="https://www.thegazette.co.uk/data" target="_blank">The
            Gazette</a>.
        </p>
        <p>
          A SPARQL endpoint provides access to RDFa and JSON-LD data formats,
          facilitating enhanced
          content discovery and recommendation features. The platform's
          capabilities include querying, visualizing,
          and recommending content based on specific metadata attributes,
          enabling users to efficiently access and manage
          a wide array of materials.
        </p>
        <p>
          Users can visualize statistics based on RDF data, with options to
          download these statistics as PNG or SVG files.
          Similarly, statistics and query results from the SPARQL endpoint can
          be downloaded in various formats, such as HTML or CSV.
          The platform also offers advanced filtering options based on language,
          date, and authors, as well as keyword searches
          across all articles. Data can be visualized in multiple formats,
          including HTML, JSON-LD, and tabular formats.
        </p>
        <p>
          Inspired by initiatives such as
          <a href="https://www.europeana.eu/en/stories"
            target="_blank">Europeana Stories</a> and
          <a href="https://www.thegazette.co.uk/data" target="_blank">The London
            Gazette</a>, NePr is poised to set a new standard
          in digital articles by providing robust tools for the systematic
          construction and intuitive interfacing of provenance data.
        </p>
      </section>
      <section id="structure">
        <h2>Structure of the web application</h2>

        <section id="version-control">
          <h3>Version Control</h3>
          <p>
            To effectively manage the development and history of our
            application, we are utilizing GitHub, a renowned platform for
            version control. GitHub not only allows us to keep track of all
            changes made to the codebase but also enables seamless collaboration
            among team members. The repository for this project can be found at
            <a href="https://github.com/OpariucRares/Web-News-Provenance"
              target="_blank">https://github.com/OpariucRares/Web-News-Provenance</a>.
          </p> </section>

        <section id="ui-mockup">
          <h3>UI Mockup</h3>
          <div
            style="display: grid; grid-template-rows: 1fr 1fr; grid-template-columns: 1fr 1fr;">
            <img src="./images/homePageUI.png" style="width:100%"
              alt="Home Page UI">
            <img src="./images/articleDetailsPageUI.png" style="width:100%"
              alt="Article Details Page UI">
            <img src="./images/advancedSearchPageUI.png" style="width:100%"
              alt="Advanced Search Page UI">
            <img src="./images/sparqlPageUI.png" style="width:100%"
              alt="SPARQL Page UI">
            <img src="./images/statisticsPageUI.png" style="width:100%"
              alt="Statistics Page UI">
          </div>
          <p>
            We have designed the UI/UX in Figma. The first image showcases the
            home page, where the user can view a collection of article cards.
            Users can interact by either searching for articles or viewing an
            article directly. By clicking the "View" button, they will be
            redirected to the article details page, where they can see detailed
            information about the article, including the source, image, and
            embedded content of the article (such as PDF or other formats,
            websites, etc.). At the bottom of the page, recommended articles are
            displayed.

            We also have an advanced search page, where users can apply various
            filters to search for articles based on specific criteria.
            Additionally, there is a SPARQL endpoint that can return data in
            formats like RDF and JSON-LD, and the data can be downloaded.

            Lastly, there's a statistics page that features interactive
            statistics about the data.
          </p>
        </section>

        <section id="final-ui">
          <h3>Final UI</h3>
          <div>
            <img src="./images/homePage1.png" style="width:100%"
              alt="Home Page 1">
            <img src="./images/homePage2.png" style="width:100%"
              alt="Home Page 2">
            <p>The home page displays a collection of article cards. Users can
              search for articles or view them directly. Pagination has been
              implemented to navigate through the articles.</p> </div>
          <div>
            <img src="./images/articleDetailsPage1.png" style="width:100%"
              alt="Article Details Page 1">
            <img src="./images/articleDetailsPage2.png" style="width:100%"
              alt="Article Details Page 2">
            <p>The article details page provides detailed information about the
              article, including the source, image, and embedded content (such
              as PDFs, websites, etc.). Recommended articles are displayed at
              the bottom of the page.</p> </div>
          <div>
            <img src="./images/sparqlPage.png" style="width:100%"
              alt="SPARQL page">
            <p>The advanced search page allows users to apply various filters to
              search for articles based on specific criteria such as language,
              category, author name and date. Pagination has
              been implemented to navigate through the search results.</p>
          </div>
          <div>
            <img src="./images/advancedSearchPage.png" style="width:100%"
              alt="Advanced Search page">
            <p>The SPARQL endpoint provides data in RDF, JSON-LD, and tabular
              formats. Users can download the data in various formats, including
              RDFa, CSV, and JSON-LD.</p> </div>
          <div>
            <img src="./images/statisticsPage1.png" style="width:100%"
              alt="Statistics page">
            <img src="./images/statisticsPage2.png" style="width:100%"
              alt="Statistics page 2">
            <p>The statistics page features interactive visualizations and
              statistics about the data. Statistics can be downloaded in both
              .png and .svg formats.</p> </div>
        </section>
      </section>

      <section id="system-architecture">
        <h2>System Architecture</h2>

        <section id="overview">
          <h3>Overview</h3>
          <p>In this chapter, we present the architecture of our Web News
            Provenance platform, highlighting the key components and
            technologies that enable its functionality. The system architecture
            consists of a React frontend built with TypeScript, utilizing
            Bootstrap and Material UI to provide an intuitive and responsive
            user interface. The frontend code is managed using GitHub and is
            hosted on Azure over a custom domain. The backend is a C#
            application developed with the .NET framework, which handles all
            business logic and data processing. For storing and managing
            provenance and metadata, we utilize an RDF storage solution powered
            by Apache Jena Fuseki and Apache Jena TDB, enabling efficient SPARQL
            queries. These components are seamlessly integrated to offer a
            cohesive and engaging user experience, allowing users to query,
            visualize, and interact with news articles and their detailed
            provenance information effortlessly.</p>
          <div>
            <img src="./images/systemArchitecture.png" style="width:100%"
              alt="Statistics page">
          </div>
        </section>

        <section id="frontend-design">
          <h3>Frontend</h3>

          <p> The frontend of the <strong>Web News Provenance</strong> platform
            is a pivotal component that offers users an engaging and intuitive
            interface to interact with news articles and their provenance
            information. This section delves into the significant details about
            the frontend, highlighting the technologies used, internal data
            structures, and design considerations. </p>

          <h4>Technologies Used</h4> <ul> <li><strong>React:</strong> A
              JavaScript library for building user interfaces, enabling the
              creation of reusable UI components.</li>
            <li><strong>TypeScript:</strong> A typed superset of JavaScript that
              compiles to plain JavaScript, enhancing code quality and
              maintainability through static type checking.</li>
            <li><strong>Bootstrap:</strong> A CSS framework that provides
              responsive design templates and pre-designed components, ensuring
              consistency across the application.</li> <li><strong>Material-UI
                (MUI):</strong> A React UI framework that implements Google's
              Material Design, offering a rich set of customizable
              components.</li> <li><strong>NPM:</strong> The Node Package
              Manager, used for managing project dependencies and scripts.</li>
            <li><strong>Other Libraries:</strong> Libraries such as
              <code>axios</code> for HTTP requests, <code>React Router</code>
              for routing, and various utility libraries for enhanced
              functionality.</li> </ul>

          <h4>Application Structure</h4> <p> The frontend application follows a
            modular structure, promoting scalability and maintainability. Key
            aspects include: </p> <ul> <li><strong>Component-Based
                Architecture:</strong> UI elements are encapsulated within
              components, facilitating reuse and isolation.</li>
            <li><strong>State Management:</strong> Utilization of React's
              <code>useState</code> and <code>useEffect</code> hooks for
              managing state and side effects within functional components.</li>
            <li><strong>Routing:</strong> Implementation of <code>React
                Router</code> for client-side routing, enabling seamless
              navigation between different pages.</li>
            <li><strong>Styles:</strong> Combination of Bootstrap and
              Material-UI theming for consistent styling and responsive
              design.</li> </ul>

          <h4>Key Components and Pages</h4> <p> The frontend encompasses several
            key pages, each catering to specific functionalities: </p> <ul>
            <li><strong>Home Page:</strong> Displays a collection of article
              cards with pagination and search capabilities. Offers the
              possibility to search an article based on some words.</li>
            <li><strong>Article Details Page:</strong> Shows comprehensive
              information about selected articles, including metadata and
              embedded content.</li> <li><strong>Advanced Search Page:</strong>
              Provides filters for language, category, author name, and date to
              refine search results.</li> <li><strong>SPARQL Page:</strong>
              Allows users to execute SPARQL queries and view results in various
              formats (RDFa, JSON-LD, tabular), with options to download
              data.</li>
            <li><strong>Statistics Page:</strong> Presents interactive
              visualizations of data statistics, which can be downloaded in .png
              and .svg formats.</li> </ul>

          <h4>API Integration</h4> <p> The frontend communicates with the
            backend API through RESTful endpoints using asynchronous HTTP
            requests with the native <code>fetch</code> API. Error handling
            is crucial to provide a robust user experience. </p> <p> For
            example, when fetching a paginated list of article cards, we use
            the function: </p> <ul>
            <li><code>getAllArticleCardsPagination(offset: number):
                Promise&lt;ArticleCard[] | string&gt;</code></li> </ul> <p>
            This function sends a GET request to the backend and checks the
            response: </p> <p> <code> if (!response.ok) {<br>
              &nbsp;&nbsp;throw new
              Error("Failed to fetch article cards");<br> } </code> </p> <p>
            Errors are caught and handled consistently across all API
            functions: </p> <p> <code> catch (error) {<br>
              &nbsp;&nbsp;console.error("Error:", error);<br> &nbsp;&nbsp;if
              (error instanceof Error) {<br> &nbsp;&nbsp;&nbsp;&nbsp;return
              Error: ${error.message};<br> &nbsp;&nbsp;} else {<br>
              &nbsp;&nbsp;&nbsp;&nbsp;return Error: ${String(error)};<br>
              &nbsp;&nbsp;}<br> } </code> </p> <p> By returning a promise
            that resolves to either the expected data or an error message,
            our components receive consistent feedback, allowing them to
            provide meaningful messages to the user. </p> <p> Similar
            patterns are used in other API functions, such as: </p> <ul>
            <li><code>getArticleById(articleId: string): Promise&lt;Article
                | string&gt;</code></li>
            <li><code>searchArticleCards(search: string, offset: number):
                Promise&lt;ArticleCard[] | string&gt;</code></li> </ul> <p>
            This approach ensures that error handling is centralized and
            uniform throughout the application, enhancing maintainability
            and reliability. </p>

          <h4>Styling and Theming</h4> <p> Styling is achieved through a
            combination of Bootstrap and Material-UI. To align with the
            platform's branding and ensure visual consistency, we defined custom
            colors and overrode Bootstrap classes. The primary color scheme uses
            <code>#1263b4</code> for a vibrant main color, with a darker shade
            <code>#0c4f92</code> for hover effects, and an accent color
            <code>#FFC107</code> for highlights. By customizing the
            <code>:root</code> CSS variables and overriding classes like
            <code>.navbar</code>, <code>.btn-primary</code>, and pagination
            components, we maintained a consistent look and feel throughout the
            application. </p>

          <h4>Accessibility and Responsiveness</h4> <p> The application
            emphasizes accessibility by adhering to Web Content Accessibility
            Guidelines guidelines, using
            semantic HTML elements, and ensuring keyboard navigability.
            Responsiveness is achieved through flexible grid layouts and media
            queries, providing an optimal experience across various devices and
            screen sizes. Custom styles were applied thoughtfully to enhance
            usability without compromising accessibility. </p>

          <h4>Package Management and Scripts</h4> <p> Project dependencies and
            scripts are managed using NPM. Key scripts in the
            <code>package.json</code> file include: </p> <ul>
            <li><code>"start"</code>: Runs the app in development mode.</li>
            <li><code>"build"</code>: Builds the app for production to the
              <code>build</code> folder.</li> <li><code>"test"</code>: Launches
              the test runner.</li> <li><code>"eject"</code>: Removes the single
              build dependency from the project (not recommended).</li> </ul>
          <p> To start the frontend application locally, you can use the
            following command: </p> <ul> <li><code>npm run dev</code>: Runs the
              app on <code>localhost</code> in development mode.</li> </ul> <p>
            This command sets up a local development server, making the
            application accessible at <code>http://localhost:5173</code> (or a
            different port if specified) for testing and development purposes.
          </p>

          <h4>Considerations for Linked Data Principles</h4> <p> The
            frontend
            facilitates adherence to linked data principles by: </p> <ul>
            <li>Providing means to access data in formats like JSON-LD and
              RDFa.</li> <li>Ensuring that resources are identified using
              URIs.</li> <li>Enabling users to explore data connections
              through
              SPARQL queries and results.</li> </ul>

          <h4>HTML5 Validation</h4> <p> To ensure that our application adheres
            to the highest standards of web development, we validated our HTML5
            code using the W3C Markup Validation Service available at <a
              href="https://validator.w3.org/"
              target="_blank">validator.w3.org</a>. This service checks the HTML
            code for errors and provides feedback on any issues that need to be
            addressed. </p> <p> We are pleased to report that our application
            passed the validation with no errors, confirming that our HTML5 code
            is compliant with the latest web standards. This validation is
            crucial for ensuring cross-browser compatibility, accessibility, and
            overall robustness of the application. </p> <div> <img
              src="./images/html5Validator.png" style="width:100%"
              alt="HTML5 Validation Result"> </div>

          <h4>Schema.org and RDFa Tag Validation</h4> <p> To ensure that our
            application correctly uses Schema.organd RDFa tags for semantic web
            integration, we validated our implementation using the OpenLink
            Structured Data Sniffer available at <a
              href="https://osds.openlinksw.com/"
              target="_blank">osds.openlinksw.com</a>. This tool examines the
            tags applied to elements on the frontend and provides detailed
            feedback on their usage. </p> <p> We are pleased to report that our
            application successfully uses Schema.organd RDFa constructs,
            ensuring that our data is well-structured and machine-readable. This
            validation is crucial for enhancing search engine optimization (SEO)
            and enabling richer integration with semantic web technologies. </p>
          <div> <img src="./images/snifferImage.png" style="width:100%"
              alt="Schema.org and RDFa Validation Result"> </div>
          <p> The
            thoughtful integration of these Schema.org and RDFa tags ensures
            that
            the <strong>Web News Provenance</strong> platform is semantically
            enriched and compliant with modern web standards. </p>

          <p> The thoughtful integration of these technologies and practices
            ensures that the frontend of the <strong>Web News
              Provenance</strong> platform is robust, user-friendly, and
            aligns
            with modern web development standards. </p> </section>

        <section id="backend-api">
          <h3>Backend - API Implementation</h3>
          <div>
            <p>This report outlines the implementation of the backend API for
              the WebNewsProvenance project. The API is designed to interact
              with a SPARQL endpoint to retrieve and manage news articles and
              related data. The system is composed of a set of service and query
              interfaces, data models, and controllers that work together to
              process requests, execute queries, and return appropriate
              responses. Below is a detailed breakdown of the backend's
              architecture, the implementation details of the queries, services,
              and controllers, as well as error handling and data flow.</p>
          </div>
          <h3>Project Structure</h3>
          <div>
            <p>The WebNewsProvenance API is built using a layered architecture
              that separates concerns into distinct services, queries, and
              controllers. Let's analyze the logic of the Sparql Controller</p>

            <ul>
              <li><strong>Namespace</strong>:
                <code>WebNewsProvenance.Models</code>
                <ul>
                  <li><strong>SparqlResponse</strong>: The data model that
                    standardizes the response format for the SPARQL
                    queries.</li>
                </ul>
              </li>
              <li><strong>Namespace</strong>:
                <code>WebNewsProvenance.Services.Queries.Contracts</code>
                <ul>
                  <li><strong>ISparqlQueries</strong>: The interface defines the
                    contract for various SPARQL queries needed to interact with
                    the news data. These queries include retrieving articles,
                    paginated results, articles by search, and recommended
                    articles.</li>
                </ul>
              </li>
              <li><strong>Namespace</strong>:
                <code>WebNewsProvenance.Services</code>
                <ul>
                  <li><strong>SparqlQueries</strong>: The concrete
                    implementation of <code>ISparqlQueries</code> that
                    constructs the SPARQL queries. This class handles the
                    construction of query strings by combining predefined
                    namespaces with query parameters such as pagination
                    (<code>limit</code> and <code>offset</code>), filters, and
                    search terms.</li>
                  <li><strong>ISparqlService</strong>: The interface for the
                    service that handles querying the SPARQL endpoint.</li>
                  <li><strong>SparqlService</strong>: The implementation of
                    <code>ISparqlService</code> that executes queries against
                    the endpoint and processes results.</li>
                </ul>
              </li>
              <li><strong>Namespace</strong>:
                <code>WebNewsProvenance.Controllers</code>
                <ul>
                  <li><strong>API Controllers</strong>: Expose the endpoints of
                    the API to the frontend, managing HTTP requests and
                    coordinating with the services.</li>
                </ul>
              </li>
            </ul>
          </div>

          <h3>Example Query: <code>GetAllArticlesCardPagination</code></h3>
          <pre><code>
          public string GetAllArticlesCardPagination(int limit, int offset)
          {
              return $@"
              {GetAllNamespacesQuery}
              SELECT ?article ?headline ?image ?description
              WHERE {{
                  ?article a nepr:Article ;
                          schema:headline ?headline ;
                          schema:description ?description ;
                          schema:image ?image .
              }}
              LIMIT {limit} OFFSET {offset}";
          }
          </code></pre>
          <p>In this implementation:</p>
          <ul>
            <li><strong>Namespaces</strong>: Various namespaces are predefined
              at the beginning of the class for reuse in queries.</li>
            <li><strong>Query Construction</strong>: The query is dynamically
              constructed with the <code>limit</code> and <code>offset</code>
              parameters, ensuring efficient pagination when fetching
              articles.</li>
          </ul>

          <h3>The <code>SparqlService</code> Implementation</h3>
          <p>The <code>SparqlService</code> class implements the
            <code>ISparqlService</code> interface. It acts as a middle layer
            that handles query execution and formats the responses for the
            frontend.</p>

          <h3>Constructor:</h3>
          <pre><code>
      public SparqlService(IOptions<AppSettings> options, ISparqlQueries sparqlQueries)
      {
          _fusekiEndpoint = options.Value.FusekiEndpoint;
          _sparqlQueries = sparqlQueries;
      }
          </code></pre>
          <p>The service is injected with the settings for the Fuseki endpoint
            (the SPARQL endpoint URL) and the <code>ISparqlQueries</code>
            dependency to fetch the relevant queries.</p>
          <h3>General Error Handling:</h3>
          <ul>
            <li><strong>Invalid SPARQL Query</strong>: If the query is invalid
              (throws <code>RdfQueryException</code>), the response includes an
              error message and a <code>400 BadRequest</code> status code.</li>
            <li><strong>Unsupported Format</strong>: If an unsupported format is
              requested, a <code>400 BadRequest</code> is returned.</li>
            <li><strong>General Exceptions</strong>: Any other errors result in
              a <code>500 InternalServerError</code> status code with the
              exception message.</li>
          </ul>

          <h3>Controller - API Endpoints</h3>
          <p>The API exposes several endpoints that allow clients to interact
            with the system. One such endpoint is <code>GetArticleById</code>,
            which allows fetching a single article by its ID.</p>
          <h3>Logic:</h3>
          <ul>
            <li>The method invokes the service to fetch the article.</li>
            <li>Based on the status code of the response, the method returns
              different HTTP responses:
              <ul>
                <li><code>200 OK</code> if the request is successful.</li>
                <li><code>400 BadRequest</code> if the request has invalid
                  parameters.</li>
                <li><code>404 NotFound</code> if no article is found.</li>
                <li><code>500 InternalServerError</code> if an unexpected error
                  occurs.</li>
              </ul>
            </li>
          </ul>

          <p>The system handles errors gracefully by catching exceptions and
            returning appropriate HTTP status codes. Errors are categorized as:
            <ul>
              <li><code>BadRequest</code>: For invalid SPARQL queries or
                unsupported formats.</li>
              <li><code>InternalServerError</code>: For unexpected errors or
                failures in communication with the SPARQL endpoint.</li>
              <li><code>NotFound</code>: When no article matches the provided
                ID.</li>
            </ul>
          </p>

          <p>Each error response includes a descriptive message that helps
            diagnose the issue.</p>

        </section>
        <section id="rdf-store"> <h3>RDF Store</h3> <p> In the architecture of
            our <strong>Web News Provenance</strong> platform, the RDF store
            plays a pivotal role in managing and querying the knowledge about
            the provenance of online newspaper articles. We have chosen
            <strong>Apache Jena Fuseki</strong> as our RDF store, a robust and
            scalable solution that seamlessly integrates with the broader
            semantic web infrastructure. </p> <p> <a
              href="https://jena.apache.org/documentation/fuseki2/"
              target="_blank">Apache Jena Fuseki</a> serves as
            our chosen RDF store due to its efficiency in managing and querying
            RDF data. It provides a robust and scalable platform for storing
            ontological information about online newspaper articles. The RDF
            store is a critical component, housing the structured knowledge
            derived from various sources and enriched through integration with
            <a role="doc-biblioref"
              href="https://www.dbpedia.org/resources/sparql/"
              target="_blank">DBpedia</a> and
            <a role="doc-biblioref"
              href="https://www.wikidata.org/wiki/Wikidata:Main_Page"
              target="_blank">Wikidata</a>. </p> <p> We have tested our RDF
            store locally
            using Apache Jena and created custom ontologies that recognize
            multiple prefixes to ensure seamless integration and querying
            capabilities. </p> <p> In conclusion, the use of Apache Jena Fuseki
            for our RDF store ensures that we can efficiently manage and query
            the detailed provenance information of online newspaper articles,
            providing a strong foundation for the Web News Provenance platform.
          </p> </section>

        <section id="sparql-endpoint"> <h3>SPARQL Endpoint</h3> <p> The SPARQL
            endpoint is an essential feature that enables users to interact with
            our RDF store, allowing them to query and retrieve specific data
            about newspaper articles. This functionality enhances the user
            experience by providing a powerful tool to explore the rich dataset
            stored in our RDF repository. </p>

          <p> To leverage the RDF store for hosting our ontology, we decided to
            create a custom ontology using a Python script. Although it wasn't
            strictly necessary, having a tailored ontology helps us better model
            and manage the data for online newspaper articles. This core entity
            links various properties and related entities, ensuring
            comprehensive metadata representation. </p>

          <p> Our SPARQL endpoint provides the following key features: </p> <ul>
            <li><strong>Data Retrieval:</strong> Users can perform SPARQL
              queries to extract detailed information about newspaper articles,
              including metadata, content, and provenance details.</li>
            <li><strong>Custom Queries:</strong> The flexibility of SPARQL
              empowers users to create custom queries that fit their specific
              needs, ranging from simple data lookups to complex ontology
              explorations.</li></ul>

          <h4>Ontology Structure</h4> <p> Our ontology includes several key
            prefixes to ensure effective integration and query capabilities:
            <ul> <li><code>@prefix dcterms: &lt;http://purl.org/dc/terms/&gt;
                  .</code></li> <li><code>@prefix schema:
                  &lt;http://schema.org/&gt; .</code></li> <li><code>@prefix
                  prov: &lt;http://www.w3.org/ns/prov#&gt; .</code></li>
              <li><code>@prefix skos:
                  &lt;http://www.w3.org/2004/02/skos/core#&gt; .</code></li>
              <li><code>@prefix nepr:
                  &lt;http://52.178.129.69:7008/api/Sparql/&gt; .</code></li>
            </ul> </p>

          <p> Below is an example of our custom ontology for an online newspaper
            article: <br> <code> nepr:Article a rdfs:Class, schema:CreativeWork
              ;<br> &nbsp;&nbsp;&nbsp;&nbsp;rdfs:label
              "Online Newspaper Article" ;<br>
              &nbsp;&nbsp;&nbsp;&nbsp;rdfs:comment
              "Represents an online newspaper article with metadata and multimedia content."
              ;<br> &nbsp;&nbsp;&nbsp;&nbsp;dcterms:title schema:headline ;<br>
              &nbsp;&nbsp;&nbsp;&nbsp;dcterms:creator schema:author ;<br>
              &nbsp;&nbsp;&nbsp;&nbsp;dcterms:date schema:datePublished ;<br>
              &nbsp;&nbsp;&nbsp;&nbsp;dcterms:language schema:inLanguage ;<br>
              &nbsp;&nbsp;&nbsp;&nbsp;dcterms:subject skos:Concept ;<br>
              &nbsp;&nbsp;&nbsp;&nbsp;schema:about sswt:Concept ;<br>
              &nbsp;&nbsp;&nbsp;&nbsp;schema:url schema:url ;<br>
              &nbsp;&nbsp;&nbsp;&nbsp;schema:contentUrl schema:fullWorkURL ;<br>
              &nbsp;&nbsp;&nbsp;&nbsp;schema:image schema:image ;<br>
              &nbsp;&nbsp;&nbsp;&nbsp;schema:video schema:video ;<br>
              &nbsp;&nbsp;&nbsp;&nbsp;schema:description schema:description
              ;<br> &nbsp;&nbsp;&nbsp;&nbsp;schema:publisher schema:publisher
              ;<br> &nbsp;&nbsp;&nbsp;&nbsp;schema:license schema:license ;<br>
              &nbsp;&nbsp;&nbsp;&nbsp;nepr:pages schema:numberOfPages ;<br>
              &nbsp;&nbsp;&nbsp;&nbsp;nepr:source prov:used ;<br>
              &nbsp;&nbsp;&nbsp;&nbsp;nepr:fullWorkURL schema:fullWorkURL .
            </code> </p>

          <h4>Visualization of the Ontology</h4> <p> To visualize our ontology,
            we utilized the RDF Grapher available at <a
              href="https://www.ldf.fi/service/rdf-grapher"
              target="_blank">ldf.fi/service/rdf-grapher</a>. This tool allowed
            us to generate graphical representations of our ontology, making it
            easier to understand the relationships between different entities
            and properties. </p> <div> <img src="./images/rdf-grapher.svg"
              style="width:100%" alt="Ontology Visualization"> </div>

          <p> This visualization offers a clear view of the structure and
            connections within our ontology, showcasing how data is organized
            and interlinked in the RDF store. </p> </section>

        <section id="data-collection"> <h3>Data Collection for Articles</h3>
          <p>This document explains the SPARQL queries used to retrieve data
            from Wikidata and The Gazette. The queries extract information such
            as article metadata and notice details, which are later processed
            and stored for further analysis.</p>
          <p>We used different typef of articles:</p>
          <pre><code>
            article_types = {
              "Q5707594": "news",
              "Q30070590": "magazine",
              "Q17928402": "blogs",
              "Q13433827": "encyclopedia",
              "Q13442814": "scientific"
          }
          </code></pre>
          <p>The following SPARQL query retrieves various metadata about
            articles, including title, author, date, language, subject, source,
            URL, and other relevant details.</p>
          <pre><code> 
PREFIX dcterms: <http://purl.org/dc/terms/>
PREFIX schema: <http://schema.org/>
PREFIX wd: <http://www.wikidata.org/entity/>
PREFIX wdt: <http://www.wikidata.org/prop/direct/>
PREFIX prov: <http://www.w3.org/ns/prov#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
SELECT ?article ?title ?author ?date ?language ?pages ?subject ?source ?url 
?image ?video ?description ?fullWorkURL ?creationDate ?lastModifiedDate ?revisionDate ?editor
WHERE {{
  ?article wdt:P31 wd:{article_type} ;
          rdfs:label ?title ;
          wdt:P18 ?image;
          wdt:P50 ?author ;
          wdt:P953 ?fullWorkURL.
  OPTIONAL {{ ?article schema:description ?description }}
  OPTIONAL {{ ?article wdt:P921 ?subject }}
  OPTIONAL {{ ?article wdt:P1104 ?pages }}
  OPTIONAL {{ ?article wdt:P577 ?date }}
  OPTIONAL {{ ?article wdt:P364 ?language }}
  OPTIONAL {{ ?article wdt:P1433 ?source }}
  OPTIONAL {{ ?article wdt:P856 ?url }}
  OPTIONAL {{ ?article wdt:P1651 ?video }}
  OPTIONAL {{ ?article schema:dateCreated ?creationDate }}
  OPTIONAL {{ ?article schema:dateModified ?lastModifiedDate }}
  OPTIONAL {{ ?article prov:qualifiedRevision/prov:atTime ?revisionDate }}
  OPTIONAL {{ ?article prov:qualifiedRevision/prov:agent ?editor }}
FILTER(LANG(?title) = "en" || LANG(?title) = "es")
}} LIMIT 300"""</code></pre>

          <p><strong>Explanation:</strong></p>
          <ul>
            <li><strong>PREFIX</strong>: Defines vocabularies like Dublin Core
              Terms, Schema.org, and Wikidata.</li>
            <li><strong>SELECT</strong>: Specifies fields to retrieve.</li>
            <li><strong>WHERE</strong>: Defines conditions for selecting
              articles.</li>
            <li><strong>OPTIONAL</strong>: Includes optional fields that may not
              always be available.</li>
            <li><strong>FILTER</strong>: Ensures results are in English or
              Spanish.</li>
            <li><strong>LIMIT</strong>: Restricts the output to 100,000
              results.</li>
          </ul>
          <p>The following query retrieves notices from The Gazette, including
            their title, publication date, notice code, and associated
            activities.</p>

          <pre><code>
    PREFIX gaz: <https://www.thegazette.co.uk/def/publication#>
    PREFIX prov: <http://www.w3.org/ns/prov#>
    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

    SELECT ?notice ?title ?publicationDate ?noticeCode ?act_label ?agent ?agentLabel
    WHERE {
      ?notice a gaz:Notice ;
              rdfs:label ?title ;
              gaz:hasPublicationDate ?publicationDate ;
              gaz:hasNoticeCode ?noticeCode ;
              prov:wasGeneratedBy ?act .

      ?act a prov:Activity ;
           rdfs:label ?act_label .

      OPTIONAL { ?act prov:wasAssociatedWith ?agent . ?agent rdfs:label ?agentLabel }
    }
    LIMIT 200
    </code></pre>

          <p><strong>Explanation:</strong></p>
          <ul>
            <li><strong>PREFIX</strong>: Defines vocabularies for Gazette
              notices and provenance data.</li>
            <li><strong>SELECT</strong>: Specifies fields to retrieve.</li>
            <li><strong>WHERE</strong>: Ensures results match Gazette
              notices.</li>
            <li><strong>OPTIONAL</strong>: Includes agent-related fields that
              may be missing.</li>
            <li><strong>LIMIT</strong>: Restricts output to 200 results.</li>
          </ul>
        </section>

        <section id="cloud-deployment">
          <h3>Deployment on Cloud</h3>
          <p>
            The application was deployed using a cloud-based infrastructure
            hosted on Azure, with both backend and frontend components hosted
            on
            the platform. Below are the details about how the deployment was
            structured, including the challenges faced, solutions implemented,
            and the integration of various services.
          </p>

          <h4>Deployment Architecture Overview</h4>
          <p>
            The backend API was deployed on a Virtual Machine (VM) in Azure.
            Apache was installed on this VM to serve the application, which
            interacted with the RDF store for data querying and storage. The
            backend API, built using .NET, was also hosted on the same VM,
            allowing the application to be accessed via the public IP of the
            VM.
          </p>
          <p>
            For the frontend, multiple deployment solutions were considered.
            The
            frontend was first hosted using GitHub Pages for its ease of
            deployment, but later moved to Azure Web App for better
            integration
            with the backend API. We also registered a custom domain
            registered
            with Name.com, allowing a professional and consistent user
            experience for future realses.
          </p>

          <h4>Domain and Access Setup</h4>
          <p>
            The backend API was made accessible through the public IP of the
            Azure VM, which exposed the service to the frontend application. A
            custom domain was configured for the frontend offered by Azure Web
            App, allowing users to access the application via a user-friendly
            URL.
          </p>

          <h4>Deployment Challenges and Solutions</h4>
          <p>
            While the backend API was deployed successfully, there were
            challenges with hosting the frontend. Initially, the application
            was
            deployed via GitHub Pages, but issues arose with securely serving
            both the backend and frontend. The backend API was being accessed
            over HTTP, which raised concerns regarding security.
          </p>
          <p>
            To address these challenges:
            <ul>
              <li><strong>Switch to Azure Web App:</strong> The frontend was
                migrated to Azure Web App, which allowed for easier
                integration
                with the backend.
                <li><strong>Continuous Deployment via GitHub Actions:</strong>
                  To ensure the frontend was always up to date with the latest
                  changes, GitHub Actions was set up for Continuous Deployment
                  (CD). This automation allowed for seamless deployment
                  whenever
                  changes were committed to the frontend repository.</li>
              </ul>
            </p>

            <h4>Future Enhancements</h4>
            <p>
              Looking ahead, several improvements are planned for scaling and
              enhancing the security of the deployment:
              <ul>
                <li><strong>Full HTTPS Setup for Backend:</strong> Currently,
                  the backend API is accessible via HTTP. Future work will
                  involve configuring SSL certificates on the Azure VM to
                  ensure
                  the backend is also accessed securely over HTTPS.</li>
                <li><strong>Scaling with Azure:</strong> As the number of
                  users
                  grows, Azure's scaling capabilities will be leveraged to
                  ensure the application can handle increased traffic without
                  performance degradation.</li>
                <li><strong>Monitoring and Alerts:</strong> Azure's monitoring
                  tools will be used to set up automated alerts for any
                  failures
                  or issues in the application, ensuring proactive maintenance
                  and a smooth user experience.</li>
              </ul>
            </p>
          </section>
        </section>
        <section id="user-guide">
          <h2>User Guide</h2>
          <p>
            Welcome to the application! Here, you can explore various articles
            on different subjects, visualize details about each article,
            search for specific content, and even interact with the SPARQL
            interface to query data.
          </p>

          <section id="browsing-articles">
            <h3>Browsing Articles</h3>
            <p>
              When you first open the application, you will be presented with a
              list of articles on various subjects. If you find an article that
              interests you, simply click the **"Visualize"** button to view
              more details about that article.
            </p>
            <div> <img src="./images/homePage1.png" style="width:100%"
                alt="Home Page"> </div>
            <p>
              If the articles displayed on the current page are not of interest
              to you, you can navigate through different pages using the
              Next and Previous buttons at the bottom of the page.
            </p>
            <div> <img src="./images/homePage3.png" style="width:100%"
                alt="Home Page 3"> </div>
            <p>
              You can also search for articles by title or description using the
              search bar at the top of the page. Simply type a keyword or
              phrase, and the application will filter and show the relevant
              articles.
            </p>
            <div> <img src="./images/homePage4.png" style="width:100%"
                alt="Home Page 4"> </div>
          </section>

          <section id="article-details-and-recommendations">
            <h3>Article Details and Recommendations</h3>
            <p>
              After clicking the "Visualize" button on an article, you will be
              taken to a detailed view of that article, including its provenance
              information.
            </p>
            <div> <img src="./images/articleDetailsPage1.png" style="width:100%"
                alt="Article Details Page 1"> </div>
            <p>
              If you scroll down to the bottom of the article page, you will
              find a section displaying recommended articles based on the
              category of the current article. These recommendations may help
              you discover other interesting content related to your current
              topic.
              <div> <img src="./images/articleDetailsPage2.png"
                  style="width:100%"
                  alt="Article Details Page 2"> </div>
            </p>
          </section>

          <section id="sparql-interface">
            <h3>SPARQL Interface</h3>
            <p>
              If you're looking for more advanced knowledge or want to explore
              data in more detail, the SPARQL page is available to assist you.
            </p>
            <div> <img src="./images/sparqlPage1.png"
                style="width:100%"
                alt="Sparql Page"> </div>
            <p>
              You can enter your SPARQL query in the input field and choose the
              desired output format (RDFa, JSON-LD, or Table). If your query is
              correct, the application will display the results accordingly.
            </p>
            <p>
              Table
            </p>
            <div> <img src="./images/sparqlPage.png"
                style="width:100%"
                alt="Sparql Page "> </div>
            <p>
              JSON-LD
            </p>
            <div> <img src="./images/sparqlPage2.png"
                style="width:100%"
                alt="Sparql Page "> </div>
            <p>
              RDFa
            </p>
            <div> <img src="./images/sparqlPage3.png"
                style="width:100%"
                alt="Sparql Page "> </div>
            <p>
              Additionally, if you want to keep the data for later use, you can
              download the results as a CSV/RDFa/JSON-LD file by clicking the
              download
              button.
            </p>
          </section>

          <section id="advanced-search">
            <h3>Advanced Search</h3>
            <p>
              The advanced search feature allows you to filter articles based on
              several conditions, including:
            </p>
            <ul>
              <li>Language</li>
              <li>Technology</li>
              <li>Author Name</li>
              <li>Start Date</li>
              <li>End Date</li>
            </ul>
            <p>
              After selecting your preferred filtering options, click
              "Apply Filters". The application will then display articles
              that match your specified criteria.
            </p>
            <div> <img src="./images/advancedSearchPage.png"
                style="width:100%"
                alt="Advanced Search Page"> </div>
          </section>

          <section id="statistics">
            <h3>Statistics</h3>
            <p>
              To gain more insight into the data you're working with, you can
              visit the Statistics page. This page will provide detailed
              statistics about the dataset, such as the number of articles,
              their categories, and other useful information.
            </p>
            <div> <img src="./images/statisticsPage1.png"
                style="width:100%"
                alt="Statistics Page"> </div>
            <p>
              This page is designed to help you understand the scope and
              structure of the available data.
            </p>
          </section>
          <section id="video-presentation">
            <h3>Video Presentation</h3>
            <iframe width="560" height="315"
              src="https://www.youtube.com/embed/6C-EiTA4_pI" frameborder="0"
              allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
              allowfullscreen></iframe>
          </section>
        </section>
        <section id="references">
          <h2>References/Bibliography</h2>
          <ul>
            <ul>
              <li><a href="https://www.dbpedia.org/resources/sparql/"
                  target="_blank">DBpedia</a></li>
              <li><a href="https://www.wikidata.org/wiki/Wikidata:Main_Page"
                  target="_blank">Wikidata</a></li>
              <li><a href="https://service.tib.eu/webvowl/"
                  target="_blank">WebVOWL</a></li>
              <li><a
                  href="https://protegewiki.stanford.edu/wiki/VOWL">VOWL</a></li>
              <li><a href="https://protege.stanford.edu/"
                  target="_blank">PROTÉGÉ</a></li>
              <li><a href="https://jena.apache.org/documentation/fuseki2/"
                  target="_blank">Apache Jena Fuseki</a></li>
              <li><a href="https://www.dublincore.org/schemas/rdfs/"
                  target="_blank">Dublin Core RDF Schema</a></li>
              <li><a
                  href="https://github.com/pgroth/PROVTutorial?tab=readme-ov-file"
                  target="_blank">PROV Tutorial</a></li>
              <li><a
                  href="https://vocabulary.semantic-web.at/semweb/export/semweb.ttl"
                  target="_blank">Semantic Web Vocabulary</a></li>
              <li><a
                  href="https://www.dublincore.org/specifications/dublin-core/dcmi-terms/dublin_core_terms.ttl"
                  target="_blank">Dublin Core Terms</a></li>
              <li><a
                  href="https://databus.dbpedia.org/ontologies/dbpedia.org/ontology--DEV"
                  target="_blank">DBpedia Ontology</a></li>
              <li><a href="https://www.thegazette.co.uk/flint"
                  target="_blank">The Gazette - Flint</a></li>
              <li><a href="https://sparql.europeana.eu/"
                  target="_blank">Europeana SPARQL Endpoint</a></li>
              <li><a href="https://dotnetrdf.github.io/"
                  target="_blank">DotNetRDF Documentation</a></li>
            </ul>
          </ul>
        </section>

      </body>
    </html>
